{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Team_SBB_project_1_Explainable_AI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4vVWKLwD5vC",
        "colab_type": "text"
      },
      "source": [
        "# eXplainable AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAhBzbrEg-Yx",
        "colab_type": "text"
      },
      "source": [
        "## What is going on in the black box?\n",
        "\n",
        "**Artificial intelligence** (AI) has been the subject of vast research and numerous applications in computer science for decades. The more recent rise in computing power has allowed the advent of new approaches of designing AI systems, notably based on machine learning techniques like **deep learning**.\n",
        "\n",
        "These machine learning models are in large part what makes modern technologies such as self-driving cars possible. While they produce deceptively accurate and helpful predictions, there is growing concern that the way these algorithms operate is *too obscure*; this has earned them the name of **black boxes**, i.e. we know what goes in and what comes out, but their inner workings are too complex for humans to grasp."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCF-j86YiUFv",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.ibb.co/8rTjCgb/1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhlphZUQhD9C",
        "colab_type": "text"
      },
      "source": [
        "## Trustworthy decisions\n",
        "\n",
        "As they are supposed to mimic (or sometimes even surpass) human intelligence, AI systems have to make numerous **decisions** as part of their processes.\n",
        "\n",
        "Given the growing importance and prevalence of AI in today's world, it is important that humans can **trust** the decisions made by AI. This need for trustworthy AI has led to the quest for **explainable AI** (XAI), which seeks to bridge this gap between AI and humans by designing methodologies and tools that can explain the behavior of artificial intelligence so that humans can understand them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oTFy72IhITP",
        "colab_type": "text"
      },
      "source": [
        "## The day AI went wrong... again\n",
        "\n",
        "It should be apparent why humans might want to place more trust in systems powered by AI. Recent news have been awash with stories of AI showing gender or racial **bias**, or failure of critical systems based on AI. **Consequences** for humans are bad enough for applications like hiring systems or credit scoring,  but they are even greater when it comes to AI use in the medical, legal or law-enforcement fields.\n",
        "\n",
        "One [example often cited in the press](https://medium.com/thoughts-and-reflections/racial-bias-and-gender-bias-examples-in-ai-systems-7211e4c166a1) is that of the software named **COMPAS**, which is used as a decision support tool in the US to help judges evaluate the possibility that criminals are going to reoffend. This AI was shown to be **strongly prejudiced** against black offenders, even when their past crimes were minor compared to those of their white counterparts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQYN5OL9hZXl",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.ibb.co/4sp8ZZx/2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgHWEJqrhakN",
        "colab_type": "text"
      },
      "source": [
        "Of course, AI systems are as good or flawed as the developers that create them and the initial data that they are trained with. As things currently stand, they are bound to make **dubious choices** because (just like humans) they can be easily deceived. Think of how [autonomous cars can be fooled](https://arxiv.org/abs/1802.06430) into mistaking some road signs for others...\n",
        "\n",
        "Beyond the trust issue, it should be noted that **legal compliance** of algorithms has started to make its way in policies as well. Thus, the [GDPR introduces in its 71st article](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679) a **right to explanation**, stating that people have a right to obtain an explanation of how a decision was reached, when that decision is based on the automatic processing of their personal data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA67sF-WhoXP",
        "colab_type": "text"
      },
      "source": [
        "## Explain?\n",
        "\n",
        "Explaining AI, or \"opening the black box\" as it were, can be done in a variety of ways. There is a first distinction to make between 1) understanding *how* models work (i.e., turn the black box into a *transparent* one) and 2) understanding *why* a model reached a particular decision (i.e., provide a post-hoc explanation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh_xr0UtkkAp",
        "colab_type": "text"
      },
      "source": [
        "The first category consists of *modifying* the models used so that every step of the machine learning process can be **interpretable**. By \"interpretability\", we mean that humans have to be able to extract relevant, insightful knowledge from the models (see [this article](https://arxiv.org/pdf/1901.04592.pdf)): how does the model train? How does it predict? What effect does changing a particular parameter have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B59JcWthsbb",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.ibb.co/qptfSbb/3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjza4BbHkfk5",
        "colab_type": "text"
      },
      "source": [
        "Since transparent systems replace traditional black box systems, they need to provide similar results (fidelity) as well as the same level of accuracy, performance and scalability offered by their black-box counterparts while having the additional burden of guaranteeing privacy, fairness, interpretability, etc. There is often a **trade-off** between performance and those extra benefits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMC9_klpkrMN",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.ibb.co/qmBMZnw/4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP9dAEYrhvtr",
        "colab_type": "text"
      },
      "source": [
        "The second category is more concerned with explaining the **final outcomes**, be it a single outcome given a particular test set (= *local*), or any outcome for any given test set (= *global*), or simply with providing a visual representation of a model’s behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke2G7LrDh6Qf",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.ibb.co/RDshqc8/5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM15SuYRh63d",
        "colab_type": "text"
      },
      "source": [
        "Thus, the explanations provided by an XAI library **vary** according to the type of approach. Libraries that work for any model are said to be *model-agnostic*.\n",
        "\n",
        "It should be noted that the results will depend on the data and the algorithms used, but most importantly the **stakeholders** and the **context**. Different people have different interests in what AI produces: a criminal might want to know if the AI has treated him fairly, whereas a CEO will be more interested in knowing whether the option suggested by the AI is the most lucrative one. However, [as F. Lécué puts it](http://www.semantic-web-journal.net/system/files/swj2198.pdf), \"the AI community is far from having self-explainable AI systems which automatically adapt to any (i) data, (ii) ML algorithm, (iii) model, (iv) user, or (v) application and (v) context.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WlAREAoiEkB",
        "colab_type": "text"
      },
      "source": [
        "## Going back to the basics\n",
        "\n",
        "The problem of black boxes and their interpretability has been exacerbated by the **complexity** of popular models like *neural networks* (DNN), which are heavily utilized in the field of AI. Other such complex models include *support vector machines* (SVM) and *tree ensembles*. But one should not forget that **more simple models** already provide some level of post-hoc explanation, albeit quite limited. For example, it is possible to visualize decision trees to observe which attributes and thresholds have been chosen. Similarly, with linear regression, provided you are using comparable features, you can learn about the importance of weights given to each feature.\n",
        "\n",
        "Since these basic examples are not satisfactory, we now turn to libraries that have been especially designed for the purpose of XAI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5NftMoPnN45",
        "colab_type": "text"
      },
      "source": [
        "## LIME\n",
        "\n",
        "*- Can I trust my model?*\n",
        "\n",
        "*- LIME will tell you everything!*\n",
        "\n",
        "**LIME** (*Local Interpretable Model-Agnostic Explanations*) is a framework used in artificial intelligence and deep learning.\n",
        "\n",
        "Rather than defeat humans, LIME is designed to help them make the best **decisions**. Its main advantage is the ability to explain and interpret the results of models using text, tabular and image data. As such, it seeks to **empower** professionals. \n",
        "\n",
        "The framework, which is open source, is written in Python and R. In order to use it, you simply need to install it like a standard R or Python package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fv3oC7L2u1Z",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.ibb.co/5Tz34wq/6.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOykMzXDES9s",
        "colab_type": "text"
      },
      "source": [
        "### How does it work?\n",
        "\n",
        "If you want interpretable results, LIME also requires an interpretable representation of the *input*, i.e. one which is understandable by humans.\n",
        "\n",
        "After it has worked its magic, LIME outputs a list of explanations, reflecting the contribution of each feature to the prediction of a data sample. This provides **local interpretability**, and it also allows to determine which feature changes have the biggest impact on predictions.\n",
        "\n",
        "Let's take the example of an image. The first task is to select the **region of interest**, i.e. the area for which you wish to find an explanation to the black-box prediction. Then, the datasets are **perturbed** and **permutated** and you get black-box predictions for the new data points. **Weights** are then assigned to the new samples according to the proximity of their instance of interest. So, the relative weights can either contribute to the prediction or be evidence against it. As the next step, a weighted, interpretable model is **trained** on the dataset with variations.\n",
        "\n",
        "Finally, the predictions are explained by interpreting the local model.\n",
        "\n",
        "The **general approach** LIME takes to achieve this goal is as follows:\n",
        "\n",
        "1. For each prediction to explain, permute the observation *n* times.\n",
        "\n",
        "2. Let the complex model predict the outcome for all permuted observations.\n",
        "\n",
        "3. Calculate the distance from each permutation to the original observation.\n",
        "\n",
        "4. Convert the distance to a similarity score.\n",
        "\n",
        "5. Select *m* features best describing the complex model outcome from the permuted data. \n",
        "\n",
        "6. Fit a simple model to the permuted data, explaining the complex model outcome with the m features from the permuted data weighted by their similarity to the original observation.\n",
        "\n",
        "7. Extract the feature weights from the simple model and use these as explanations for the complex models local behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4uBX_XlE33x",
        "colab_type": "text"
      },
      "source": [
        "### Applications\n",
        "\n",
        "The interpretability of models has proven its value in many areas of business and scientific activities, such as:\n",
        "\n",
        "* **Detection of unusual behaviours**: identification and explanation of events and unusual behaviours\n",
        "* **Fraud detection**: to identify and explain why certain transactions are treated as frauds\n",
        "* **Opinion on loans and credits**: identification and explanation of why a certain customer will be able to pay off debt and another will not\n",
        "* **Marketing / targeted ads**: increasing the accuracy of offers and messages and matching the content to the most important interests\n",
        "* **Medical diagnostics**\n",
        "* **Automation of vehicles**\n",
        "* Etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y86mtvmCng2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}