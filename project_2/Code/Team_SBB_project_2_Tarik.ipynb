{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_Project2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XaviJunior/SBB/blob/master/project_2/Code/Team_SBB_project_2_Tarik.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV0gvQ7uQMEi",
        "colab_type": "text"
      },
      "source": [
        "# Real or Not? NLP with Disaster Tweets (Kaggle Competition)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isAf6gt2QMEj",
        "colab_type": "code",
        "colab": {},
        "outputId": "4e77d0ed-0e77-4b95-e0c0-82bfb02019d2"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo(\"\", width=600)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x1e3cb872d08>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vRx6pKbQMEp",
        "colab_type": "text"
      },
      "source": [
        "[Link to the GitHub repository](https://github.com/XaviJunior/SBB)\n",
        "\n",
        "[Link to the YouTube video]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRmGOkUaQMEp",
        "colab_type": "text"
      },
      "source": [
        "## Contributions\n",
        "\n",
        "* **Xavier AEBY**: \n",
        "* **Tarik BACHA**: \n",
        "* **Tanguy BERGUERAND**: \n",
        "* **Frederic SPYCHER**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1R-CUBkQMEq",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "For our second group project, we were tasked with joining the [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) Kaggle competition, which consists of devising a machine learning model that can predict, using natural language processing, whether tweets announcing a disaster are genuine or not.\n",
        "\n",
        "The incentive for such a model is to assist disaster relief organizations and news agencies identify actual   as they monitor Twitter (which is often used as a communication tool during such events)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg1haoZMQMEq",
        "colab_type": "text"
      },
      "source": [
        "## Setting things up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "woxj_8wFQMEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install spacy\n",
        "# !pip install xgboost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zYDZHlTULj-Q",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "\n",
        "RSEED = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsaWpItpQME0",
        "colab_type": "text"
      },
      "source": [
        "## The data\n",
        "\n",
        "The data provided by Kaggle contains more than **10,000 tweets**, for 70% of which we are given their `target` class (1 = true, 0 = false). Each observation from the training data is composed of an `id` and the `text` of the tweet. In some cases, a `keyword` as well as the tweet's `location` are also given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LyVmGGwCNTf7",
        "colab": {},
        "outputId": "a181f928-5675-4506-a325-3a3ff051d919"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/XaviJunior/SBB/master/project_2/Data/train.csv\", encoding=\"utf-8\")\n",
        "df.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGdfYgemQME4",
        "colab_type": "code",
        "colab": {},
        "outputId": "7bb7495f-cef6-4d13-f185-66c0e056af5f"
      },
      "source": [
        "print(\"Classified observations:\", df.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classified observations: 7613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgHmYLH7QME7",
        "colab_type": "code",
        "colab": {},
        "outputId": "e13c5307-79d3-417a-f929-0bbf00ee8099"
      },
      "source": [
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/XaviJunior/SBB/master/project_2/Data/test.csv\", encoding=\"utf-8\")\n",
        "df_test.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRj20cJqQMFB",
        "colab_type": "code",
        "colab": {},
        "outputId": "802690f7-da3a-4ada-fc18-75a7eb83d011"
      },
      "source": [
        "print(\"Unclassified observations:\", df_test.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unclassified observations: 3263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNol47kWQMFE",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peKRyVV6QMFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(tweet):\n",
        "    tweet = re.sub(r\" \\w{1,3}\\.{3,3} http\\S{0,}\", \" \", tweet)\n",
        "    tweet = re.sub(r\"http\\S{0,}\", \" \", tweet)\n",
        "    tweet = re.sub(r\".Û.\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\s+\", \" \", tweet)\n",
        "    tweet = re.sub(r'\\t', \" \", tweet)\n",
        "    tweet = re.sub(r'\\n', \" \", tweet)\n",
        "    tweet = re.sub(r\"\\.{3,3}\", \"\", tweet)\n",
        "\n",
        "    return tweet\n",
        "    \n",
        "df[\"text\"] = df[\"text\"].apply(clean)\n",
        "df_test[\"text\"] = df_test[\"text\"].apply(clean)\n",
        "\n",
        "# df[\"text\"].to_csv(\"tweets.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EETZ4eBHQMFH",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6lz7FQ-QMFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = English()\n",
        "punctuation = string.punctuation\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "def tokenizer(tweet):\n",
        "    tokens = nlp(tweet)\n",
        "    tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n",
        "    tokens = [word for word in tokens if word not in stop_words and word not in punctuation]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# vectorizer = TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1,1))\n",
        "vectorizer = CountVectorizer(tokenizer=tokenizer, ngram_range=(1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvETgb-ZQMFM",
        "colab_type": "text"
      },
      "source": [
        "## Training models\n",
        "\n",
        "For our first attempts at building a model, we went back to models previously seen in the Data Mining & Machine Learning and Big-Scale Analytics courses, without tweaking the hyperparamaters too much, in order to compare how they each perform with this particular dataset.\n",
        "\n",
        "Preparing the data: needs to be in the form of the list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8bR5ERBmNt5Y",
        "colab": {},
        "outputId": "3ea652cb-02ef-4e81-daf0-fe44f1b88bec"
      },
      "source": [
        "X = vectorizer.fit_transform(df[\"text\"].values.tolist())\n",
        "y = df[\"target\"].values.tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RSEED)\n",
        "\n",
        "X_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<6090x17410 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 51831 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNzrNnzeQMFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vectorizer.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x21Eb0cXQMFV",
        "colab_type": "text"
      },
      "source": [
        "### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzGm98AEQMFW",
        "colab_type": "code",
        "colab": {},
        "outputId": "1470ddec-68b4-4402-b5ac-5d13d71f2d4d"
      },
      "source": [
        "RF = RandomForestClassifier(criterion=\"entropy\", n_estimators=30, max_depth=200, random_state=RSEED)\n",
        "RF.fit(X_train, y_train)\n",
        "f1_score(y_test, RF.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6904109589041096"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFfZTlkGQMFZ",
        "colab_type": "text"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xdqAgLGKQMFZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "307dcac2-9d72-417d-d4c7-eab1ea0617f2"
      },
      "source": [
        "xgc = xgb.XGBClassifier(n_estimators=101, max_depth=71, base_score=0.5, objective='binary:logistic', random_state=RSEED)\n",
        "xgc.fit(X_train, y_train)\n",
        "f1_score(y_test, xgc.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7341176470588235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAJ3l4g_QMFc",
        "colab_type": "text"
      },
      "source": [
        "### Neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxDEqEmlQMFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFCxXIwXQMFh",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEWsNTShQMFi",
        "colab_type": "code",
        "colab": {},
        "outputId": "1446f2c1-6f0e-403e-8592-44244dd82f12"
      },
      "source": [
        "LR = LogisticRegressionCV(solver=\"lbfgs\", cv=5, max_iter=1000, random_state=RSEED)\n",
        "LR.fit(X_train, y_train)\n",
        "f1_score(y_test, LR.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7469084913437758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKk-5Jo4QMFo",
        "colab_type": "text"
      },
      "source": [
        "## Exporting predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1aYwMlLkNoCf",
        "colab": {}
      },
      "source": [
        "To send our submissions for the Kaggle competition, we compute predictions for a set of tweets provided by the website and send them in a CSV file containing the tweet `id` and the predicted `target` "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYSEXkK2QMFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = vectorizer.transform(df_test[\"text\"].values.tolist())\n",
        "df_test[\"target\"] = LR.predict(X)  # modify with appropriate model\n",
        "\n",
        "df_test[[\"id\", \"target\"]].to_csv(\"UNIL_SBB_FSP.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shseE6WVQMFx",
        "colab_type": "text"
      },
      "source": [
        "## Unused code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NEbpZvzvPd5s",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "text_clf = Pipeline([\n",
        "('vect', CountVectorizer()),\n",
        "('tfidf', TfidfTransformer()),\n",
        "('clf',MultinomialNB()),])\n",
        "\n",
        "text_clf.fit(Text_train, y_train)\n",
        "print(text_clf.score(Text_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFzNLRo5UnNL",
        "colab": {}
      },
      "source": [
        "# first neural network with keras tutorial\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "from keras import layers\n",
        "\n",
        "input_dim = bow_train.shape[1] \n",
        "model = Sequential()\n",
        "model.add(layers.Dense(600, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(200))\n",
        "model.add(layers.Dense(150))\n",
        "model.add(layers.Dense(10))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6PY_a8llZb36",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "               optimizer='adam', \n",
        "               metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VxBUTkqTb7Nd",
        "outputId": "ab8a9e46-9cf3-494a-f53c-f3589da5dc07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_48 (Dense)             (None, 512)               9479680   \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 50)                25650     \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 30)                1530      \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 20)                620       \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 9,507,501\n",
            "Trainable params: 9,507,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nYczaSB6Uw2X",
        "outputId": "f3e5f289-3e56-49a9-c4da-75049ac79884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.fit(bow_train, y_train, epochs=10, batch_size=50)\n",
        "loss, accuracy = model.evaluate(bow_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0063 - accuracy: 0.9952\n",
            "Epoch 2/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0061 - accuracy: 0.9947\n",
            "Epoch 3/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0064 - accuracy: 0.9949\n",
            "Epoch 4/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0062 - accuracy: 0.9956\n",
            "Epoch 5/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0064 - accuracy: 0.9954\n",
            "Epoch 6/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0063 - accuracy: 0.9952\n",
            "Epoch 7/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0064 - accuracy: 0.9949\n",
            "Epoch 8/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0065 - accuracy: 0.9954\n",
            "Epoch 9/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0064 - accuracy: 0.9952\n",
            "Epoch 10/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0065 - accuracy: 0.9947\n",
            "Testing Accuracy:  0.7525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zg-gEhTFWz4I",
        "outputId": "92c0c26e-b256-4076-ed72-f65ca0c8928e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(bow_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy:  0.7663\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}