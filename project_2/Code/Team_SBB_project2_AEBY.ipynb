{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Team_SBB_project_2_Disaster_tweets.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLy2Fs8z7fHp",
        "colab_type": "text"
      },
      "source": [
        "# Real or Not? NLP with Disaster Tweets (Kaggle Competition)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbeocVx67fHr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "c290ae19-4e03-45af-9ee6-398bd531e70c"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo(\"\", width=600)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"600\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f5af1fbbf60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiZ5bbgt7fHw",
        "colab_type": "text"
      },
      "source": [
        "[Link to the GitHub repository](https://github.com/XaviJunior/SBB)\n",
        "\n",
        "[Link to the YouTube video]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5krQkm_d7fHx",
        "colab_type": "text"
      },
      "source": [
        "## Contributions\n",
        "\n",
        "* **Xavier AEBY**: \n",
        "* **Tarik BACHA**: \n",
        "* **Tanguy BERGUERAND**: \n",
        "* **Frederic SPYCHER**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRfoK_gb7fHy",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "For our second group project, we were tasked with joining the [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) Kaggle competition, which consists of devising a machine learning model that can predict, using natural language processing, whether tweets announcing a disaster are genuine or not.\n",
        "\n",
        "The incentive for such a model is to assist disaster relief organizations and news agencies identify actual   as they monitor Twitter (which is often used as a communication tool during such events)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9id3pakT7fHy",
        "colab_type": "text"
      },
      "source": [
        "## Setting things up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BVwHdG_07fHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install spacy\n",
        "# !pip install xgboost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zYDZHlTULj-Q",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "\n",
        "RSEED = 42"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuh-Wb907fH8",
        "colab_type": "text"
      },
      "source": [
        "## The data\n",
        "\n",
        "The data provided by Kaggle contains more than **10,000 tweets**, for 70% of which we are given their `target` class (1 = true, 0 = false). Each observation from the training data is composed of an `id` and the `text` of the tweet. In some cases, a `keyword` as well as the tweet's `location` are also given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LyVmGGwCNTf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "c8b1db25-029b-4f5a-9c42-be651d9f44a8"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/XaviJunior/SBB/master/project_2/Data/train.csv\", encoding=\"utf-8\")\n",
        "df.head(3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sakSyoWN7fH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "8f8f44c3-4982-4900-99ef-598acde4caf6"
      },
      "source": [
        "print(\"Classified observations:\", df.shape[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classified observations: 7613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRJMjYN37fID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "a6c656d7-7bb0-4029-8beb-3b246d68920b"
      },
      "source": [
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/XaviJunior/SBB/master/project_2/Data/test.csv\", encoding=\"utf-8\")\n",
        "df_test.head(3)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGzpE7Wl7fII",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a40e10c7-323e-44a1-9da1-fb69d3a9d59a"
      },
      "source": [
        "print(\"Unclassified observations:\", df_test.shape[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unclassified observations: 3263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23WT1Loy7fIL",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "594qdFgw7fIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(tweet):\n",
        "    tweet = re.sub(r\" \\w{1,3}\\.{3,3} http\\S{0,}\", \" \", tweet)\n",
        "    tweet = re.sub(r\"http\\S{0,}\", \" \", tweet)\n",
        "    tweet = re.sub(r\".Û.\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\s+\", \" \", tweet)\n",
        "    tweet = re.sub(r'\\t', \" \", tweet)\n",
        "    tweet = re.sub(r'\\n', \" \", tweet)\n",
        "    tweet = re.sub(r\"\\.{3,3}\", \"\", tweet)\n",
        "\n",
        "    return tweet\n",
        "    \n",
        "df[\"text\"] = df[\"text\"].apply(clean)\n",
        "df_test[\"text\"] = df_test[\"text\"].apply(clean)\n",
        "\n",
        "# df[\"text\"].to_csv(\"tweets.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is19Wwtn7fIN",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWAbyUl_7fIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = English()\n",
        "punctuation = string.punctuation\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "def tokenizer(tweet):\n",
        "    tokens = nlp(tweet)\n",
        "    tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n",
        "    tokens = [word for word in tokens if word not in stop_words and word not in punctuation]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# vectorizer = TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1,1))\n",
        "vectorizer = CountVectorizer(tokenizer=tokenizer, ngram_range=(1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfULp_8774P2",
        "colab_type": "text"
      },
      "source": [
        "##Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5sQYzZd8Iy9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "9eb0c802-bb3a-4919-93af-1c7f4f7145f0"
      },
      "source": [
        "#Import all the dependencies\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LCoL-3N8Usr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = df_test\n",
        "\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UkUfULg9Jkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a517ff58-85b6-4f38-ddd1-d9b4cd0c72f5"
      },
      "source": [
        "max_epochs = 100\n",
        "vec_size = 20\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1)\n",
        "  \n",
        "model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "\n",
        "model.save(\"d2v.model\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
            "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 0\n",
            "iteration 1\n",
            "iteration 2\n",
            "iteration 3\n",
            "iteration 4\n",
            "iteration 5\n",
            "iteration 6\n",
            "iteration 7\n",
            "iteration 8\n",
            "iteration 9\n",
            "iteration 10\n",
            "iteration 11\n",
            "iteration 12\n",
            "iteration 13\n",
            "iteration 14\n",
            "iteration 15\n",
            "iteration 16\n",
            "iteration 17\n",
            "iteration 18\n",
            "iteration 19\n",
            "iteration 20\n",
            "iteration 21\n",
            "iteration 22\n",
            "iteration 23\n",
            "iteration 24\n",
            "iteration 25\n",
            "iteration 26\n",
            "iteration 27\n",
            "iteration 28\n",
            "iteration 29\n",
            "iteration 30\n",
            "iteration 31\n",
            "iteration 32\n",
            "iteration 33\n",
            "iteration 34\n",
            "iteration 35\n",
            "iteration 36\n",
            "iteration 37\n",
            "iteration 38\n",
            "iteration 39\n",
            "iteration 40\n",
            "iteration 41\n",
            "iteration 42\n",
            "iteration 43\n",
            "iteration 44\n",
            "iteration 45\n",
            "iteration 46\n",
            "iteration 47\n",
            "iteration 48\n",
            "iteration 49\n",
            "iteration 50\n",
            "iteration 51\n",
            "iteration 52\n",
            "iteration 53\n",
            "iteration 54\n",
            "iteration 55\n",
            "iteration 56\n",
            "iteration 57\n",
            "iteration 58\n",
            "iteration 59\n",
            "iteration 60\n",
            "iteration 61\n",
            "iteration 62\n",
            "iteration 63\n",
            "iteration 64\n",
            "iteration 65\n",
            "iteration 66\n",
            "iteration 67\n",
            "iteration 68\n",
            "iteration 69\n",
            "iteration 70\n",
            "iteration 71\n",
            "iteration 72\n",
            "iteration 73\n",
            "iteration 74\n",
            "iteration 75\n",
            "iteration 76\n",
            "iteration 77\n",
            "iteration 78\n",
            "iteration 79\n",
            "iteration 80\n",
            "iteration 81\n",
            "iteration 82\n",
            "iteration 83\n",
            "iteration 84\n",
            "iteration 85\n",
            "iteration 86\n",
            "iteration 87\n",
            "iteration 88\n",
            "iteration 89\n",
            "iteration 90\n",
            "iteration 91\n",
            "iteration 92\n",
            "iteration 93\n",
            "iteration 94\n",
            "iteration 95\n",
            "iteration 96\n",
            "iteration 97\n",
            "iteration 98\n",
            "iteration 99\n",
            "Model Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOVMDRkN9Mcx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "74ff8bee-a4dd-43fd-d8f2-e30596abda71"
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec\n",
        "\n",
        "model= Doc2Vec.load(\"d2v.model\")\n",
        "#to find the vector of a document which is not in training data\n",
        "test_data = word_tokenize(\"Hurrican on Atlanta Downtown\".lower())\n",
        "v1 = model.infer_vector(test_data)\n",
        "print(\"V1_infer\", v1)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "V1_infer [-0.01947895 -0.02376492  0.0030167   0.01074679 -0.02430422  0.01346538\n",
            " -0.02337107  0.01762431  0.01169459  0.02275454 -0.01689866  0.01578643\n",
            "  0.01763516 -0.0246702   0.00828856  0.02426583  0.01428954  0.01518091\n",
            " -0.00655923  0.00577711]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBaWsdWYBxhR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c7917bf6-5474-4016-c75f-fe26db2a76fa"
      },
      "source": [
        "# find most similar doc from the test data\n",
        "mSimilar = model.docvecs.most_similar(positive=[model.infer_vector(test_data)],topn=5)\n",
        "print(\"top 5 similar\", mSimilar)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top 5 similar [('1', 0.4186919331550598), ('3', 0.31006813049316406), ('2', 0.09603632986545563), ('0', 0.014837533235549927)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Woaq08Yu-OQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "69d2c1c0-c953-44f8-eef9-b0e5ee4164ef"
      },
      "source": [
        "# to find most similar doc using tags\n",
        "similar_doc = model.docvecs.most_similar('1')\n",
        "print(similar_doc)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('3', 0.2288547158241272), ('2', 0.06427853554487228), ('0', -0.3581893742084503)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph3SZ2Rj-Qf7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "30bddb2a-d0fb-4dfa-9dc4-e5edaa17a44e"
      },
      "source": [
        "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
        "print(model.docvecs['1'])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.02110824 -0.02011216 -0.00740983  0.02324364 -0.02904659 -0.02733524\n",
            "  0.02049188 -0.01868427  0.00615    -0.00229033 -0.00763815  0.02084077\n",
            "  0.00971974 -0.00894801  0.01055051  0.02820239 -0.00146638  0.00967818\n",
            "  0.00186863  0.02681717]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcLrGpwW7fIQ",
        "colab_type": "text"
      },
      "source": [
        "##Training models\n",
        "\n",
        "For our first attempts at building a model, we went back to models previously seen in the Data Mining & Machine Learning and Big-Scale Analytics courses, without tweaking the hyperparamaters too much, in order to compare how they each perform with this particular dataset.\n",
        "\n",
        "Preparing the data: needs to be in the form of the list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8bR5ERBmNt5Y",
        "colab": {},
        "outputId": "35eb9c81-8ad5-41db-974d-c372f0371d32"
      },
      "source": [
        "X = vectorizer.fit_transform(df[\"text\"].values.tolist())\n",
        "y = df[\"target\"].values.tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RSEED)\n",
        "\n",
        "X_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<6090x17410 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 51831 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRxK1ZMQ7fIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vectorizer.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVb6oTqN7fIV",
        "colab_type": "text"
      },
      "source": [
        "### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oWFn3BC7fIW",
        "colab_type": "code",
        "colab": {},
        "outputId": "b49cfaa4-60d3-476c-8ec6-644cdafcf7c6"
      },
      "source": [
        "RF = RandomForestClassifier(criterion=\"entropy\", n_estimators=30, max_depth=200, random_state=RSEED)\n",
        "RF.fit(X_train, y_train)\n",
        "f1_score(y_test, RF.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6904109589041096"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvrkc3Xl7fIY",
        "colab_type": "text"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "k5J-BAah7fIZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "7879ed33-390d-4e7d-fe64-b1634b54b7dc"
      },
      "source": [
        "xgc = xgb.XGBClassifier(n_estimators=101, max_depth=71, base_score=0.5, objective='binary:logistic', random_state=RSEED)\n",
        "xgc.fit(X_train, y_train)\n",
        "f1_score(y_test, xgc.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7341176470588235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bSzMlw27fIb",
        "colab_type": "text"
      },
      "source": [
        "### Neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfJSe7_N7fIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seFWz0wi7fIe",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhte6nkp7fIg",
        "colab_type": "code",
        "colab": {},
        "outputId": "04ff6f38-8a4a-4eb0-bf04-1b3612d0e8b6"
      },
      "source": [
        "LR = LogisticRegressionCV(solver=\"lbfgs\", cv=5, max_iter=1000, random_state=RSEED)\n",
        "LR.fit(X_train, y_train)\n",
        "f1_score(y_test, LR.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7469084913437758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqmbCTJZ7fIj",
        "colab_type": "text"
      },
      "source": [
        "## Exporting predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1aYwMlLkNoCf",
        "colab": {}
      },
      "source": [
        "To send our submissions for the Kaggle competition, we compute predictions for a set of tweets provided by the website and send them in a CSV file containing the tweet `id` and the predicted `target` "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUAPoS9H7fIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = vectorizer.transform(df_test[\"text\"].values.tolist())\n",
        "df_test[\"target\"] = LR.predict(X)  # modify with appropriate model\n",
        "\n",
        "df_test[[\"id\", \"target\"]].to_csv(\"UNIL_SBB_FSP.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc22b7P57fIo",
        "colab_type": "text"
      },
      "source": [
        "## Unused code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NEbpZvzvPd5s",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "text_clf = Pipeline([\n",
        "('vect', CountVectorizer()),\n",
        "('tfidf', TfidfTransformer()),\n",
        "('clf',MultinomialNB()),])\n",
        "\n",
        "text_clf.fit(Text_train, y_train)\n",
        "print(text_clf.score(Text_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFzNLRo5UnNL",
        "colab": {}
      },
      "source": [
        "# first neural network with keras tutorial\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "from keras import layers\n",
        "\n",
        "input_dim = bow_train.shape[1] \n",
        "model = Sequential()\n",
        "model.add(layers.Dense(600, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(200))\n",
        "model.add(layers.Dense(150))\n",
        "model.add(layers.Dense(10))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6PY_a8llZb36",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "               optimizer='adam', \n",
        "               metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VxBUTkqTb7Nd",
        "outputId": "ab8a9e46-9cf3-494a-f53c-f3589da5dc07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_48 (Dense)             (None, 512)               9479680   \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 50)                25650     \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 30)                1530      \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 20)                620       \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 9,507,501\n",
            "Trainable params: 9,507,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nYczaSB6Uw2X",
        "outputId": "f3e5f289-3e56-49a9-c4da-75049ac79884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model.fit(bow_train, y_train, epochs=10, batch_size=50)\n",
        "loss, accuracy = model.evaluate(bow_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0063 - accuracy: 0.9952\n",
            "Epoch 2/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0061 - accuracy: 0.9947\n",
            "Epoch 3/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0064 - accuracy: 0.9949\n",
            "Epoch 4/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0062 - accuracy: 0.9956\n",
            "Epoch 5/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0064 - accuracy: 0.9954\n",
            "Epoch 6/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0063 - accuracy: 0.9952\n",
            "Epoch 7/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0064 - accuracy: 0.9949\n",
            "Epoch 8/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0065 - accuracy: 0.9954\n",
            "Epoch 9/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0064 - accuracy: 0.9952\n",
            "Epoch 10/10\n",
            "6090/6090 [==============================] - 20s 3ms/step - loss: 0.0065 - accuracy: 0.9947\n",
            "Testing Accuracy:  0.7525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zg-gEhTFWz4I",
        "outputId": "92c0c26e-b256-4076-ed72-f65ca0c8928e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(bow_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy:  0.7663\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}